{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Mobile Robotics Final Project\n",
    "**Basics of Mobile Robotics (MICRO-452)**\n",
    "\n",
    "**Group 35 :**\n",
    "\n",
    "CARRETTI Julie\\\n",
    "DOANE Ashton\\\n",
    "BOUGUECHA Meriam\\\n",
    "SCHLUCHTER Julien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "- [Introduction - Environment Desciption](#introduction)\n",
    "- [Computer Vision](#Computer-Vision)\n",
    "    - [1. Installs for all required elements](#1.-Installs-for-all-required-elements)\n",
    "    - [2. Import and Setup](#2.-Import-and-Setup)\n",
    "    - [3. Thymio Vision Class Methods](#3.-Thymio-Vision-Class-Methods)\n",
    "    - [4. Example usage of this class](#4.-Example-usage-of-this-class)\n",
    "- [Global Navigation](#Global-Navigation)\n",
    "- [Local Navigation](#Local-Navigation)\n",
    "- [Filtering](#Filtering)\n",
    "- [Motion Control](#Motion-Control)\n",
    "- [Main](#Main)\n",
    "- [Sources](#Sources)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction - Environment Desciption\n",
    "\n",
    "A 2D map was created on an A0-sized white sheet, featuring printed shapes to represent global obstacles and 3D boxes for local obstacles. The map is discretized into 1920 by 1080 pixels, corresponding to the camera’s resolution. The goal is indicated by a green circle, while the Thymio robot is identified through a blue dot and its front yellow lights, both detected by the camera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision\n",
    "\n",
    "This section shows the steps that have gone into using computer vision for this project. We namely implemented 4 features:\n",
    "\n",
    "1. <b>Image capture </b> - Processing images using open-cv and AUKEY camera.\n",
    "2. <b>Image Filtering</b> - Smoothing out images using filtering methods such as median or gaussian filters.\n",
    "3. <b>Edge Detection and Processing </b> - Using canny edge detection and radon/hough transform to develop shapes on the map.\n",
    "4. <b>Thymio Pose Detection</b> - Detecting the location of the Thymio using pattern matching.\n",
    "5. <b>Map Generation</b> - Creating the map that will be used for path planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installs for all required elements\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade tdmclient\n",
    "!python3 -m pip install --upgrade opencv-python\n",
    "!python3 -m pip install --upgrade numpy\n",
    "!python3 -m pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import and Setup\n",
    "------\n",
    "\n",
    "The libraries used for this program:\n",
    "\n",
    "1) <b>OpenCV</b> - This library was used to perform basic image processing tasks, such as image capturing, edge detection, \n",
    "2) <b>numpy</b> - This library is essential for manipulating images from OpenCV, as these images are represented as numpy arrays.\n",
    "3) <b>matplotlib</b> - The library allows plotting within the jupyter ide, which is useful for both testing and displaying results for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # used for image processing\n",
    "import numpy as np # used for array manipulation in conjuction with\n",
    "import matplotlib.pyplot as plt # used for displaying processing steps for your aid!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Thymio Vision Class Methods\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera Calibration\n",
    "In order to properly have a scale of the environment, we choose to control the camera position relative to the environment. The implementation here is very straightfoward. Note that we have mapped the corners to a section of A0 paper, creating a known ratio of 1px = 0.75 mm. This will be used as a known ratio to map the Thymio's pixel position to a position in real space, which can then be used in the <u>filtering module</u> along with the wheel speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrateCameraPos():\n",
    "        \"\"\"\n",
    "        Position the camera such that it aligns with the corners of A0 paper as shown. This\n",
    "        is purely for user setup, and does not return a value. If dots are aligned with the corners of\n",
    "        A0 paper, ensures that 1 px = 0.9344 mm\n",
    "        \"\"\"\n",
    "        cv2.namedWindow(\"Camera Calibration\") \n",
    "        vc = cv2.VideoCapture(0)\n",
    "        ret = True\n",
    "        while True:\n",
    "            ret, frame = vc.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # add calibration circles to frame\n",
    "            cv2.circle(frame, (360, 90), 5, (0, 0, 255), 5)\n",
    "            cv2.circle(frame, (frame.shape[1]-360, 90), 5, (0, 0, 255), 5)\n",
    "            cv2.circle(frame, (360, frame.shape[0]-90), 5, (0, 0, 255), 5)\n",
    "            cv2.circle(frame, (frame.shape[1]-360, frame.shape[0]-90), 5, (0, 0, 255), 5)\n",
    "\n",
    "            cv2.imshow(\"Camera Calibration\", frame)\n",
    "            key = cv2.waitKey(50)\n",
    "            if key == ord('c'): # Escape and return image on c\n",
    "                break\n",
    "        \n",
    "        vc.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera Capture\n",
    "Simply a command to capture a frame on command. This is used to initialize the robot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captureImageOnCommand(cam_num):\n",
    "        \"\"\"\n",
    "        Provides the user with a camera feed, from which the user may input 'C' to\n",
    "        capture the image provided. Does not complete without user input.\n",
    "        @param cv2 BGR image, from which we extract edges.\n",
    "        @returns cv2 grayscale image with detected edges from input img.\n",
    "        \"\"\"\n",
    "        cv2.namedWindow(\"Camera View\")\n",
    "        vc = cv2.VideoCapture(cam_num)\n",
    "        ret = True\n",
    "        while True:\n",
    "            ret, frame = vc.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            cv2.imshow(\"Camera View\", frame)\n",
    "            key = cv2.waitKey(50)\n",
    "            if key == ord('c'): # Escape and return image on c\n",
    "                break\n",
    "        \n",
    "        vc.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Detection\n",
    "\n",
    "Edge detection has two main steps - filtering and an edge detection algortithm.\n",
    "\n",
    "1. <u>Filtering</u>: In this project, the median filter was used with a radius of 31 pixels. The motivation for this is as follows:\n",
    "-  The median filter takes the median grayscale value of the square of pixels around the target point. \n",
    "\n",
    "- Compared to other filters, the median filter has the main advantage here of eliminating ALL noise, as opposed to something like the average filter, which still factors in noise.\n",
    "\n",
    "- Our setup consists of objects with very severe and consistent (meaning that the) transitions from light to dark. The median filter ensures that any edges in the original image which only span a few pixels will be ignored, ensuring that we do not read edges from other things in the environment (i.e. the thymio is ignored).\n",
    "\n",
    "2. <u>Edge Detection</u> In this project, the canny edge detection method was used. The motivation for this is as follows:\n",
    "- We are not constrained much with respect to time for this computation. It is relatively quick, and thus we may use the more complex canny method to get a higher quality edge map.\n",
    "- Canny connects strong edges through weak edges - essentially it is a more robust way to ensure our obstacles are closed shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEdges(img, filter = 'median', edge_method = 'canny', verbose=False):\n",
    "        \"\"\"\n",
    "        Extract detected edges from a provided image.\n",
    "        @param img(cv2 BGR_image): Image from which we extract edges.\n",
    "        @param filter (string): Indication of what type of filter to overlay on the image.\n",
    "        @param edge_method (string): Indication of what type of edge detection method should be used.\n",
    "        @param verbose (bool): If true, will display each step of the processing.\n",
    "        @returns cv2 grayscale image with detected edges from input img.\n",
    "        \"\"\"\n",
    "        # First, convert the input image to grayscale\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply the selected filter\n",
    "        if filter == 'median':\n",
    "            filtered_img =  cv2.medianBlur(gray_img, 31)\n",
    "        elif filter == 'average':\n",
    "            pass\n",
    "        elif filter == 'gaussian':\n",
    "            pass\n",
    "        else:\n",
    "            filtered_img = gray_img\n",
    "\n",
    "        #Apply the selected edge detection method to the filtered image\n",
    "        if edge_method == 'canny':\n",
    "            edges = cv2.Canny(filtered_img, 100,200)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # If verbose selected, Display images\n",
    "        if verbose:\n",
    "            # Set up plot size\n",
    "            plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "            plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.1, \n",
    "                    hspace=0.4)\n",
    "            \n",
    "            #Grayscaled Image:\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.title(\"Grayscale\")\n",
    "            plt.imshow(gray_img, cmap='gray')\n",
    "\n",
    "            #Filtered Image:\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.title(\"Filtered: \" + filter)\n",
    "            plt.imshow(filtered_img, cmap='gray')\n",
    "\n",
    "            #Edges + expansion radius Image:\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.title(\"Edges: \" + edge_method)\n",
    "            plt.imshow(edges, cmap='gray')\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "        return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pose Detection\n",
    "Pose detection requires both location of the (x,y) position of the robot, as well as a heading angle which we call theta. \n",
    "\n",
    "1. <u>Template Matching for (x,y) Positioning</u>: \n",
    "\n",
    "    In order to determine the position of the Thymio, a blue dot was attached above the hole in the Thymio. A picture of this can be found under Templates/blueDot.png. We used this picture as a template for the template matching algorithm provided by OpenCV. This function essentially tests the similarilty of every MxN grid within an image to the template image provided, returning the maximally matching location.\n",
    "\n",
    "    As we used a colored dot, we chose to use the <b>TM_CCORR_NORMED</b> method. As we are using color as the main distinguisher locating the Thymio, it is important to have the blue color match strongly influence the outcome. For other methods such as TM_SQDIFF_NORMED or TM_CCOEFF, each innacurate color channel can create a large difference, with the accuracy of the blue channel being ignored. Thus, calculating instead a correlation value ensures that highly matching colors are rewarded strongly. This held true under testing, with TM_CCORR_NORMED giving the most accurate results \n",
    "\n",
    "    Two major drawbacks of this method are <b> rotation invariance </b> and <b> scale invariance </b>, meaning that the matching algorithm does not respond well to rotations or scaling of the template image. \n",
    "\n",
    "    To overcome these drawbacks, two choices were made. First, we chose to make the template a isotropically colored circle. This almost completely removes the problem of rotation invariance, as the template looks the same from all angles. However, note that the template is still required to be a square, and thus the part of the template not covered by the circle is still rotation invariant, but using a full white background seemed to avoid this problem.\n",
    "\n",
    "2. <u>Color BitMasking for Heading Angle</u>: While determining the angle could be done many ways, we opted to use the front LED on the Thymio in order to avoid any unecessary add-ons to the Thymio. As the environment is very color controlled, there should be no other orange lighting in the frame with high intensity. Thus, we used a bitmask using HSV color scheme. This allowed us to accurately select for only orange colors, with high saturation and high intensity.\n",
    "\n",
    "3. <u>Thymio Pose Detection</u>: This simply combines the prior two functions, and ensures that the results are reasonable (i.e. the center and front are not more than a thymio radius apart). Returns (x ,y, theta) prediction for the Thymio state.\n",
    "\n",
    "4. <u>Conversions from pixels to real space</u>: From before, we said we know the ratio is 1 px = 0.75mm within a bounding box we place on the screen. Thus, we simply transform from pixels to real space by adding an offset, and multiplying by 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectBlueDot(frame, divisions=3, method = 'TM_CCORR_NORMED', templatePath = \"Templates/blueDot2.png\", verbose=False):\n",
    "    \"\"\"\n",
    "    Note: Does NOT support TM_SQDIFF or SQDIFF_NORMED\n",
    "    \"\"\"\n",
    "    template = cv2.imread(templatePath) #read template as bgr image\n",
    "    globalMax = 0\n",
    "    best_approx = ([], 0, 0, 0) #pos/w/h/scale\n",
    "    \n",
    "    # resize the template image to a variety of scales, perform matching \n",
    "    for scale in np.linspace(0.5, 2.0, divisions)[::-1]:\n",
    "        resized = cv2.resize(frame, (0,0), fx=scale, fy=scale) #resize copy\n",
    "        \n",
    "        # get effective size of rectangle bounding box we are searching\n",
    "        w, h, c = template.shape\n",
    "        w = int(w/scale)\n",
    "        h = int(h/scale)\n",
    "\n",
    "        meth = getattr(cv2, method)\n",
    "\n",
    "        # Apply template Matching\n",
    "        res = cv2.matchTemplate(resized,template,meth)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "    \n",
    "        if  max_val > globalMax:\n",
    "            globalMax = max_val\n",
    "            best_approx = ([int(max_loc[0]/scale), int(max_loc[1]/scale)], w, h, scale)\n",
    "\n",
    "\n",
    "    top_left,w,h,scale = best_approx\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "    if verbose:\n",
    "        copy = frame.copy()\n",
    "        cv2.rectangle(copy, top_left, bottom_right, (255, 50, 255), 5)\n",
    "        plt.subplot(122),plt.imshow(copy,cmap = 'gray')\n",
    "        plt.show()\n",
    "    \n",
    "    x = top_left[0] + int(w/2)\n",
    "    y = top_left[1] + int(h/2)\n",
    "    return (x,y) #return center of box\n",
    "\n",
    "def detectOrangeHeading(frame, reduction = 0.1, THRESHOLD = 50, verbose = False):\n",
    "    lower_quality = cv2.resize(frame, (0,0), fx = reduction, fy = reduction) # rescale for faster processing\n",
    "    # Create a mask that looks for only the light indicator for position\n",
    "    hsv = cv2.cvtColor(lower_quality, cv2.COLOR_BGR2HSV) #convert to hsv for masking\n",
    "    lower_orange = np.array([0, 0, 230]) # hue/saturation/brightness\n",
    "    upper_orange = np.array([180, 255, 255]) \n",
    "    mask = cv2.inRange(hsv, lower_orange, upper_orange)\n",
    "    result = cv2.bitwise_and(lower_quality, lower_quality, mask=mask) # image correcting\n",
    "\n",
    "    centerX = 0\n",
    "    centerY = 0\n",
    "    numDataPoints = 0\n",
    "    for i, row in enumerate(result):\n",
    "        for j, pixel in enumerate(row):\n",
    "            if pixel.any() != 0:\n",
    "                centerX += j\n",
    "                centerY += i\n",
    "                numDataPoints += 1\n",
    "    if numDataPoints < THRESHOLD:\n",
    "        if verbose: \n",
    "            print(\"Orange not found\")\n",
    "        return (None, None)\n",
    "    centerX = int(centerX/numDataPoints/reduction) # find average and rescale to full value\n",
    "    centerY = int(centerY/numDataPoints/reduction)\n",
    "    \n",
    "    # If verbose, display each step of processing\n",
    "    if verbose:\n",
    "        plt.subplot(121), plt.imshow(result)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(122), plt.imshow(rgb)\n",
    "        plt.plot([centerX], [centerY], 'o')\n",
    "        plt.show()\n",
    "    pass\n",
    "\n",
    "    return (centerX, centerY)\n",
    "        \n",
    "def getThymioPose(frame, verbose=False):\n",
    "        \"\"\"\n",
    "        Extracts the Thymio pose from a camera feed and returns as a triple of (x,y,theta), relative to the top-left corner of the camera.\n",
    "        @param frame (np.array): BGR cv2 image to extract position from.\n",
    "        @returns (x, y, theta, size)\n",
    "        \"\"\"\n",
    "        relevantFrame = frame[90:-90, 360:-360]\n",
    "        blueX, blueY = ThymioVision.detectBlueDot(relevantFrame, minScale=0.5, maxScale=0.5, divisions=1)\n",
    "        orangeX, orangeY = ThymioVision.detectOrangeHeading(relevantFrame, reduction=0.3, THRESHOLD=15, filter_size=10)\n",
    "\n",
    "        if blueX is None or orangeX is None:\n",
    "            print('Thymio not found.')\n",
    "            return (None, None, None)\n",
    "        \n",
    "        blueX += 360\n",
    "        blueY += 90\n",
    "        orangeX += 360\n",
    "        orangeY += 90\n",
    "\n",
    "        dx = float(orangeX-blueX)\n",
    "        dy = -float(orangeY-blueY)\n",
    "        theta = np.arctan2(dy,dx)\n",
    "\n",
    "        if np.sqrt(dx**2 + dy**2 ) > 300:\n",
    "            print('Thymio not found. Distance too large')\n",
    "            return (None, None, None)\n",
    "\n",
    "        if verbose:\n",
    "            plt.rcParams[\"figure.figsize\"] = (16,4)\n",
    "            plt.imshow(frame)\n",
    "            plt.quiver(blueX, blueY, dx, dy, color='red')\n",
    "            plt.show()\n",
    "        return (blueX, blueY, theta)\n",
    "\n",
    "\n",
    "def pixelToRealSpace(position):\n",
    "        \"\"\"\n",
    "        Converts a pixel location to a dimension in real space. Coordinate frame centered on the top left corner of the paper.\n",
    "        As the setup always ensures alignment of the camera to the corners of A0 paper, the ratio is set.\n",
    "        @param position (x,y): Pixel location on the camera image.\n",
    "        @returns (x,y) tuple of location in real space in cm.\n",
    "        \"\"\"\n",
    "        # Camera shape (1080, 1920, 3)\n",
    "        # Paper dimensions (841 x 1189mm)\n",
    "        # Alignment from calibration such that 1 px = 0.9344 mm\n",
    "        # return ((position[0]-360)*0.9344/10, (position[1]-90)*0.9344/10)\n",
    "        return ((position[0]-360)*0.416/10,(position[1]-90)*0.416/10) #TEMPORARY SETUP NOV29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal Detection\n",
    "Refer to the above template matching scheme for reasoning of implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectGoal(frame, divisions=4, method = 'TM_CCORR_NORMED', templatePath = \"Templates/greenDot2.png\", verbose=False):\n",
    "        \"\"\"\n",
    "        Note: Does NOT support TM_SQDIFF or SQDIFF_NORMED\n",
    "        \"\"\"\n",
    "        template = cv2.imread(templatePath) #read template as bgr image\n",
    "        globalMax = 0\n",
    "        best_approx = ([], 0, 0, 0) #pos/w/h/scale\n",
    "        \n",
    "        # resize the template image to a variety of scales, perform matching \n",
    "        for scale in np.linspace(0.5, 2, divisions)[::-1]:\n",
    "            resized = cv2.resize(frame, (0,0), fx=scale, fy=scale) #resize copy\n",
    "            \n",
    "            # get effective size of rectangle bounding box we are searching\n",
    "            w, h, c = template.shape\n",
    "            w = int(w/scale)\n",
    "            h = int(h/scale)\n",
    "\n",
    "            meth = getattr(cv2, method)\n",
    "\n",
    "            # Apply template Matching\n",
    "            res = cv2.matchTemplate(resized,template,meth)\n",
    "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "        \n",
    "            if  max_val > globalMax:\n",
    "                globalMax = max_val\n",
    "                best_approx = ([int(max_loc[0]/scale), int(max_loc[1]/scale)], w, h, scale)\n",
    "\n",
    "\n",
    "        top_left,w,h,scale = best_approx\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "        if verbose:\n",
    "            copy = frame.copy()\n",
    "            plt.rcParams[\"figure.figsize\"] = (16,4)\n",
    "            cv2.rectangle(copy, top_left, bottom_right, (255, 50, 255), 5)\n",
    "            plt.imshow(copy,cmap = 'gray')\n",
    "            plt.show()\n",
    "        \n",
    "        x = top_left[0] + int(w/2)\n",
    "        y = top_left[1] + int(h/2)\n",
    "        return (x,y) #return center of box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Generation\n",
    "\n",
    "Map generation simply used the previously known data in order to generate a map that could be parsed by the path planning module. This simply requires running edge detection on a frame, and then expanding each edge with a radius equal to the Thymio radius. This ensures that the path we follow not only avoids edges with the center of the robot, but actually keeps the entire robot safe from obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMap(frame, verbose=False):\n",
    "        \"\"\"\n",
    "        Determine the map of the layout by considering thymio position, size, detected edges, and goal position. \n",
    "        @param frame (np.array): A camera image\n",
    "        @returns Tuple (map, start, goal) with types (np.array, [x,y], [x,y]) representing the map of edges, start location\n",
    "        and goal position for the A* algorithm.\n",
    "        \"\"\"\n",
    "        # Get edge list\n",
    "        edges = ThymioVision.getEdges(frame)\n",
    "\n",
    "        # Find start and goal position\n",
    "        startPos = ThymioVision.detectBlueDot(frame)\n",
    "        if not startPos:\n",
    "            print(\"Thymio start position not found\")\n",
    "        tSize =  100 #50 # based on the calibrated camera, the thymio can be approximated by this radius\n",
    "        goalPos = ThymioVision.detectGoal(frame)\n",
    "        if not goalPos:\n",
    "            print(\"Goal not found\")\n",
    "            return\n",
    "\n",
    "\n",
    "        # clear out space around goal and start\n",
    "        cv2.circle(edges, startPos, radius=int(tSize), thickness=-1, color=0)\n",
    "        cv2.circle(edges, goalPos, radius=int(tSize), thickness=-1, color=0)\n",
    "        #radius\n",
    "        final_map = np.zeros(shape=edges.shape)\n",
    "        for i, row in enumerate(edges):\n",
    "            for j, pixel in enumerate(row):\n",
    "                if pixel == 255:\n",
    "                    cv2.circle(final_map, (j,i), radius=int(tSize), thickness=-1, color=1)\n",
    "\n",
    "        return (final_map, startPos, goalPos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Example usage of this class \n",
    "\n",
    "Please refer to the example video linked to see the outputs of these methods.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.ThymioVision import ThymioVision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First apply the calibration filter in order to align the camera\n",
    "ThymioVision.calibrateCameraPos(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture a frame of interest.\n",
    "frame = ThymioVision.captureImageOnCommand(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, procure edges from camera capture \n",
    "edges = ThymioVision.getEdges(frame, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = ThymioVision.getThymioPose(frame, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = ThymioVision.getMap(frame, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test color matching from image to locate thymio\n",
    "ThymioVision.detectOrangeHeading(frame, verbose=True, reduction=0.3, THRESHOLD=1, filter_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.ThymioVision import ThymioVision\n",
    "# Example usage for live thymio tracking\n",
    "import cv2\n",
    "cv2.namedWindow(\"Camera View\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = vc.read()\n",
    "    if ret:\n",
    "        x1, y1, t = ThymioVision.getThymioPose(frame)\n",
    "        print(ThymioVision.pixelToRealSpace((x1,y1)), t)\n",
    "        # x1, y1 = detectOrangeHeading(frame, reduction=0.3, THRESHOLD=1, filter_size=10, verbose=False)\n",
    "        cv2.circle(frame, (x1,y1), 5, (0,0,255), -1)\n",
    "        cv2.imshow(\"Camera View\", frame)\n",
    "    key = cv2.waitKey(50)\n",
    "    if key == ord('c'): # Escape and return image on c\n",
    "        break\n",
    "\n",
    "vc.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Planning\n",
    "\n",
    "For the path planning component, we employed **cell decomposition** combined with the **Best-First Search** strategy, which led us to implement the A* algorithm.\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "The A* pathfinding algorithm was developed by following the pseudocode provided in the lecture *\"Space Robotics Systems\"*, delivered by Prof. Antonio Genova and Dr. Edoardo Del Vecchio from the Department of Mechanical and Aerospace Engineering, Sapienza University of Rome. During the development process, I used **ChatGPT** by OpenAI and **GitHub Copilot** to help refine the implementation, improve the code structure, and add meaningful comments. These tools provided suggestions that I adapted to meet the specific requirements of this project. This acknowledgment ensures transparency and helps prevent concerns related to plagiarism.\n",
    "\n",
    "\n",
    "### Implementation Overview\n",
    "To implement the A* algorithm, we followed this pseudocode:\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/pseudocode_Astar.jpg\" alt=\"A* Pseudocode\" style=\"width:40%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import heapq\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NODE Class Overview\n",
    "\n",
    "The `NODE` class is designed to represent a point in the grid, encapsulating all the necessary attributes and methods for implementing the A* pathfinding algorithm. Below, we describe the key components of the `NODE` class and their roles:\n",
    "\n",
    "#### **Attributes and Methods**\n",
    "\n",
    "- `position`: Represents the coordinates of the node on the grid (e.g., `[x, y]`).\n",
    "\n",
    "- `g`: The **cost** from the starting node to the current node. It is initialized to infinity (`float('inf')`) to indicate that nodes start off as unexplored.\n",
    "\n",
    "- `h`: The **heuristic estimate** of the cost from the current node to the goal. This value guides the A* algorithm toward the goal.\n",
    "\n",
    "- `f`: The **total cost**, computed as `f = g + h`. This value is used to prioritize nodes during the search.\n",
    "\n",
    "- `parent`: A reference to the **parent node** in the path, used for path reconstruction.\n",
    "\n",
    "- **Comparison (`__lt__`)**:\n",
    "This method allows nodes to be compared based on their f value. It is crucial because we use heapq, a priority queue that requires comparison for efficient sorting.\n",
    "\n",
    "- **String Representation (`__repr__`)**:\n",
    "This method defines how the node should be represented when printed.\n",
    "\n",
    "- **Path Reconstruction (`reconstruct_path`)**:\n",
    "This static method is used to reconstruct the optimal path once the goal is reached. Starting from the goal node, it traces back through each node’s parent attribute until it reaches the start, thereby creating the complete path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NODE:\n",
    "    def __init__(self, position, g=float('inf'), h=0, parent=None):\n",
    "        self.position = position\n",
    "        self.g = g\n",
    "        self.h = h\n",
    "        self.f = g + h\n",
    "        self.parent = parent\n",
    "\n",
    "    # Nodes will be compared based on their f value, we use this because of the heapq\n",
    "    def __lt__(self, other):\n",
    "        return self.f < other.f\n",
    "\n",
    "    # Function to represent the node\n",
    "    def __repr__(self):\n",
    "        return f\"Node(position={self.position}, g={self.g}, h={self.h}, f={self.f})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct_path(node):\n",
    "        path = []\n",
    "        while node:\n",
    "            path.append(node.position)\n",
    "            node = node.parent\n",
    "        return path[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Function: Octile Distance\n",
    "\n",
    "The heuristic function used in this implementation is based on the **Octile distance**, which is suitable when diagonal movement is allowed (resulting in 8 possible movement directions). This heuristic ensures an efficient estimation of the shortest path by considering both orthogonal and diagonal moves.\n",
    "The values assigned to these movements are:\n",
    "- Vertical/Horizontal Movement Cost = 1\n",
    "- Diagonal Movement Cost = $\\sqrt{2} \\approx 1.414$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic function using Octile distance (allowing 8 directions of movement)\n",
    "def Heuristic_function(node, goal):\n",
    "    dx = abs(node.position[0] - goal.position[0])\n",
    "    dy = abs(node.position[1] - goal.position[1])\n",
    "    return max(dx, dy) + (math.sqrt(2) - 1) * min(dx, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A* Pathfinding Algorithm\n",
    "\n",
    "The `A_star` function is the core of our path planning, implementing the A* search algorithm to find an optimal path from a start to a goal position on a grid-based map. Below, we provide a breakdown of how the algorithm operates:\n",
    "\n",
    "#### Explanation of the Key Steps\n",
    "\n",
    "- **Initialization**:\n",
    "  - **Start and Goal Nodes**: The function begins by creating a start node (`START_NODE`) with `g = 0` (the cost to reach the start node is zero) and a goal node (`GOAL_NODE`).\n",
    "  - **Heuristic Calculation**: The heuristic (`h`) for the start node is computed using the `Heuristic_function`.\n",
    "  - **Data Structures**:\n",
    "    - **Open List** (`OPEN_list`): A priority queue that stores nodes yet to be explored, sorted based on their `f` value (total estimated cost).\n",
    "    - **Closed Set** (`CLOSED_set`): A set that tracks nodes that have already been explored.\n",
    "    - **Nodes Dictionary** (`nodes`): Tracks nodes that have already been visited, ensuring that if a node is revisited, the algorithm always uses the shortest known path to reach it rather than simply using the most recent path.\n",
    "\n",
    "- **Environment Matrix Representation**:\n",
    "  - The **environment map** is represented as a matrix where:\n",
    "    - **`0`** indicates a **free path** that the algorithm can traverse.\n",
    "    - **`1`** indicates an **obstacle** that cannot be crossed.\n",
    "\n",
    "- **Search Loop**:\n",
    "  - The algorithm proceeds until the **open list** is empty. It repeatedly pops the node with the **lowest `f` value** from the open list.\n",
    "  - If the **current node** is already in the closed set, it is skipped to avoid redundant processing. Otherwise, it is added to the closed set for tracking purposes.\n",
    "\n",
    "- **Goal Check**:\n",
    "  - If the current node's position matches the goal, the function reconstructs the **optimal path** using the `reconstruct_path()` method.\n",
    "\n",
    "- **Exploration of Neighboring Nodes**:\n",
    "  - **Moves**: The possible moves include 8 directions — vertical, horizontal, and diagonal.\n",
    "  - For each potential move:\n",
    "    - The **neighbor's position** is calculated, and checks are performed to ensure that it is within bounds and is not an obstacle.\n",
    "    - If the position is already in the closed set, it is skipped.\n",
    "    - The **movement cost** is calculated:\n",
    "      - **Vertical/Horizontal Movement Cost** = 1\n",
    "      - **Diagonal Movement Cost** = $\\sqrt{2} \\approx 1.414$\n",
    "  - The **tentative cost (`g_tentative`)** to reach the neighbor is computed by adding the movement cost to the `g` value of the current node.\n",
    "\n",
    "- **Neighbor Node Handling**:\n",
    "  - If the neighbor node does not exist in the `nodes` dictionary, a **new node** is created.\n",
    "  - If the newly found path to the neighbor is **more efficient** than any previously known path (i.e., has a lower `g` value), the neighbor node's values are updated to reflect this improvement.\n",
    "  - The neighbor node is then added to the **open list** for future exploration.\n",
    "\n",
    "- **No Path Found**:\n",
    "  - If the **open list** is exhausted without reaching the goal, the function returns **`None`** and prints \"No path found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_star(environment_map, start_position, goal_position):\n",
    "\n",
    "    # Create the start and goal nodes\n",
    "    start_node = NODE(start_position, g=0)\n",
    "    goal_node = NODE(goal_position)\n",
    "\n",
    "    # Set the heuristic value for the start node\n",
    "    start_node.h = Heuristic_function(start_node, goal_node)\n",
    "    start_node.f = start_node.g + start_node.h\n",
    "\n",
    "    # Initialize the open and closed lists\n",
    "    open_list = []\n",
    "    heapq.heappush(open_list, start_node)\n",
    "    closed_set = set()\n",
    "\n",
    "    # Dictionary to keep track of nodes\n",
    "    nodes = {}\n",
    "    nodes[tuple(start_node.position)] = start_node\n",
    "\n",
    "    while open_list:\n",
    "        # Pop the node with the lowest f value\n",
    "        current_node = heapq.heappop(open_list)\n",
    "\n",
    "        # If the current node is in the closed set, skip it\n",
    "        if tuple(current_node.position) in closed_set:\n",
    "            continue\n",
    "\n",
    "        # Add the current node's position to the closed set\n",
    "        closed_set.add(tuple(current_node.position))\n",
    "\n",
    "        # If the current node is the goal, reconstruct the path\n",
    "        if current_node.position == goal_node.position:\n",
    "            path = NODE.reconstruct_path(current_node)\n",
    "            return path\n",
    "\n",
    "        # Possible moves: 8 directions (including diagonals)\n",
    "        moves = [[1, 0], [0, 1], [-1, 0], [0, -1],\n",
    "                 [1, 1], [-1, -1], [1, -1], [-1, 1]]\n",
    "\n",
    "        # Explore neighbors\n",
    "        for move in moves:\n",
    "            neighbor_position = [current_node.position[0] + move[0],\n",
    "                                 current_node.position[1] + move[1]]\n",
    "            neighbor_pos = tuple(neighbor_position)\n",
    "\n",
    "            # Skip if out of bounds or obstacle\n",
    "            if (neighbor_position[0] < 0 or neighbor_position[0] >= environment_map.shape[0] or\n",
    "                neighbor_position[1] < 0 or neighbor_position[1] >= environment_map.shape[1] or\n",
    "                environment_map[neighbor_position[0], neighbor_position[1]] == 1):\n",
    "                continue\n",
    "\n",
    "            # Skip if in closed set\n",
    "            if neighbor_pos in closed_set:\n",
    "                continue\n",
    "\n",
    "            # Calculate movement cost (diagonal movement cost is sqrt(2))\n",
    "            dx = abs(move[0])\n",
    "            dy = abs(move[1])\n",
    "            movement_cost = math.sqrt(2) if dx == 1 and dy == 1 else 1\n",
    "\n",
    "            # Calculate tentative g value\n",
    "            g_tentative = current_node.g + movement_cost\n",
    "\n",
    "            # Create or get the neighbor node\n",
    "            if neighbor_pos not in nodes:\n",
    "                neighbor_node = NODE(neighbor_position)\n",
    "                nodes[neighbor_pos] = neighbor_node\n",
    "            else:\n",
    "                neighbor_node = nodes[neighbor_pos]\n",
    "\n",
    "            # If this path to neighbor is better, record it\n",
    "            if g_tentative < neighbor_node.g:\n",
    "                neighbor_node.g = g_tentative\n",
    "                neighbor_node.h = Heuristic_function(neighbor_node, goal_node)\n",
    "                neighbor_node.f = neighbor_node.g + neighbor_node.h\n",
    "                neighbor_node.parent = current_node\n",
    "\n",
    "                # Add the neighbor to the open list\n",
    "                heapq.heappush(open_list, neighbor_node)\n",
    "\n",
    "    print(\"No path found.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Finding the Treasure\n",
    "\n",
    "To illustrate how our A* pathfinding algorithm works, we present a simple example: the goal is to reach the **treasure** in the shortest possible path.\n",
    "\n",
    "In the following grid map:\n",
    "\n",
    "- The **starting point** is marked by a **pirate boat**, and the **goal** is to reach the **treasure**.\n",
    "- The algorithm efficiently navigates through free paths while avoiding obstacles to determine the optimal route.\n",
    "\n",
    "Below is an illustration of the grid:\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/maze_map_example.png\" alt=\"Maze map\" style=\"width:40%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_binary_array(image_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    image = image.convert(\"L\")\n",
    "\n",
    "    # Set the binary threshold\n",
    "    binary_threshold = 200\n",
    "\n",
    "    # Convert grayscale to binary image\n",
    "    bw_image = image.point(lambda p: p < binary_threshold and 1)\n",
    "\n",
    "    # Convert image to numpy array\n",
    "    binary_matrix = np.array(bw_image)\n",
    "    return binary_matrix\n",
    "\n",
    "def swap_path_coordinates(path_coordinates):\n",
    "    # Swap the x and y coordinates of the path (to match the image)\n",
    "    swapped_path = [[y, x] for x, y in path_coordinates]\n",
    "    return swapped_path\n",
    "\n",
    "def display_path(maze_name, path):\n",
    "    # Load the maze image\n",
    "    maze_path = maze_name\n",
    "    maze_img = Image.open(maze_path)\n",
    "\n",
    "    # Convert the maze image to RGB (to draw in color)\n",
    "    maze_img = maze_img.convert(\"RGB\")\n",
    "\n",
    "    # Swap the x and y coordinates of the path (to match the image)\n",
    "    path = swap_path_coordinates(path)\n",
    "\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(maze_img)\n",
    "\n",
    "    # Draw the path (red line or points)\n",
    "    for i in range(len(path) - 1):\n",
    "        x1, y1 = path[i]\n",
    "        x2, y2 = path[i + 1]\n",
    "        draw.line((x1, y1, x2, y2), fill=(255, 0, 0), width=4)  # Red line with width 2\n",
    "\n",
    "    # Save and show the image\n",
    "    maze_img.save(\"maze_with_path.png\")\n",
    "    maze_img.show()\n",
    "    return\n",
    "\n",
    "# Test the A* algorithm\n",
    "\n",
    "# Define the start and goal positions\n",
    "start_position = [340, 120]\n",
    "goal_position = [310, 793]\n",
    "\n",
    "# Load the binary matrix from the image\n",
    "binary_matrix = convert_image_to_binary_array(\"maze_map_example.png\")\n",
    "\n",
    "# Find the path using A*\n",
    "path = A_star(binary_matrix, start_position, goal_position)\n",
    "\n",
    "# Display the path on the maze image\n",
    "display_path(\"maze_map_example.png\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "The resulting path is shown below, marked in **red** to indicate the shortest route taken by the pirate boat to reach the treasure.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/maze_with_path.png\" alt=\"maze with path\" style=\"width:40%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Navigation\n",
    "\n",
    "We initially tried to implement a wall-following technique for obstacle avoidance, but due to the nature of our map—specifically the narrow paths the Thymio has to navigate—we opted for a different approach that suited our environment better. To handle these situations more effectively, we decided to make our Thymio “smarter” by leveraging the **global map** we defined earlier. \n",
    "\n",
    "When computing the global map, we generate a **distance map**. This distance map converts our environment into a grid of distances, where each cell represents how far it is from the nearest wall or object in the map. Essentially, it gives the Thymio a better sense of the available space around it in a precomputed, global way, rather than relying solely on its proximity sensors. This makes navigation in tight spaces much easier and more efficient.\n",
    "\n",
    "When the Thymio detects a local 3D obstacle (using its proximity sensors), it enters the **avoidance state ( state 1)**. In this state, the decision on how to avoid the obstacle is based on determining which side (left or right) has more free space. This decision is made by the `check_sides` function, which uses the distance map to evaluate the distance to obstacles on either side of the robot. Specifically, it projects left and right from the robot’s current position, checking where there is more space to turn. This is a simple yet effective way to choose the best side to maneuver toward.\n",
    "\n",
    "After determining which side has more free space, the Thymio executes a simple maneuver in the `avoid_obstacle` function: it turns toward the side with more space, reorients itself, and then moves forward to get back on track. The duration and execution of this maneuver vary depending on which proximity sensors are triggered ,their thresholds and the turning decision—larger turns are performed when the front sensors detect an obstacle. The timer values for these maneuvers were determined through empirical experimentation. Once the obstacle is cleared, the Thymio goes to **state 2** recalculates its path to the goal using our A* algorithm, ensuring it continues its journey efficiently without getting stuck.\n",
    "\n",
    "This approach allowed us to accomplish the local navigation task in a way that suited our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Thymio's position is estimated using a Kalman Filter, specifically an Exended Kalman filter due to the non-linearity of the differential-drive kinematics. Another possible option was a Particle Filter, however, it is less computationally efficient. Given the need for real-time estimation, the EKF was a better choice. \n",
    "\n",
    "The observations are given by two sources : the odometry and the camera measurements. The noise of those two sources are assumed to be Gaussian which aligns with the assumptions of the EKF. \n",
    "\n",
    "The code of the Kalman filter can be found in *Kalman.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters \n",
    "#### Input parameters\n",
    "\n",
    "| Input parameters | Descriptions | Units |\n",
    "|------------------|-----------------|-----------------|\n",
    "| `state_est_prev`   | Previous state estimate = [x_prev, y_prev, yaw_prev, v_prev, omega_prev]    | [mm, mm, rad, mm/s, rad/s]   |\n",
    "| `control_vect_prev`     | Previous control vector = [v_input_prev, omega_input_prev] |  [mm/s, rad/s]  |\n",
    "| `P_prev` | Previous state a posteriori covariance | [mm $^2$/s $^2$] |\n",
    "| `obs_camera`   | Observation vector from the camera = [x_cam, y_cam, yaw_cam]    | [mm, mm, rad]   |\n",
    "| `obs_odometry`   | Observation vector from the odometry = [v_odo, omega_odo]    | [mm/s, rad/s]   |\n",
    "| `camera_state`   | State of the camera = True if available, False otherwise    | [Bool]   |\n",
    "| `Ts`   | Time step   | [s]  |\n",
    "\n",
    "#### Output parameters\n",
    "| Output parameters | Descriptions | Units |\n",
    "|------------------|-----------------|-----------------|\n",
    "| `state_est`   | Current state estimate = [x_est, y_est, yaw_est, v_est, omega_est]    | [mm, mm, rad, mm/s, rad/s]   |\n",
    "| `P_est` | Current state a posteriori covariance | [mm $^2$/s $^2$] |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space state estimation \n",
    "The EKF was implemented with the help of :\n",
    "- \"Lesson 7/8- Uncertainties\" from the course Basics of Mobile Robotics (MICRO-452) given by Francesco Mondada.\n",
    "- Automatic Addison https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/\n",
    "- Wikipedia page of the Extended Kalman Filter : https://en.wikipedia.org/wiki/Extended_Kalman_filter#Discrete-time_predict_and_update_equations\n",
    "\n",
    "\n",
    "In the EKF, the current state is estimated from the previous one using the dynamic of the model: \n",
    "\n",
    "$$\n",
    "state_{k} = \\begin{bmatrix} \n",
    "x_{k} \\\\\n",
    "y_{k} \\\\\n",
    "\\gamma_{k} \\\\\n",
    "v_{k}\\\\\n",
    "\\omega_{k}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "x_{k-1} + \\cos(\\gamma_{k-1}) \\cdot T_s \\cdot v_{k-1}\\\\\n",
    "y_{k-1} + \\sin(\\gamma_{k-1}) \\cdot T_s \\cdot v_{k-1}\\\\\n",
    "\\gamma_{k-1} + Ts \\cdot \\omega_{k-1}\\\\\n",
    "v_{input}\\\\\n",
    "\\omega_{input}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Here, $v_{input}$ and $\\omega_{input}$ are the command given to the Thymio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted covariance \n",
    "The predicted covariance is given by :\n",
    "$$\n",
    "P_{pred} =  G\\cdot P_{k-1}\\cdot G^T + Q\n",
    "$$\n",
    "\n",
    "The matrix $G$ is given by the Jacobian the motion model :\n",
    "\n",
    "$$\n",
    "G = \\begin{bmatrix}\n",
    "1 & 0 & -\\sin(\\gamma_{k-1}) \\cdot T_s \\cdot v_{k-1} & \\cos(\\gamma_{k-1}) \\cdot T_s & 0 \\\\\n",
    "0 & 1 & \\cos(\\gamma_{k-1}) \\cdot T_s \\cdot v_{k-1} & \\sin(\\gamma_{k-1}) \\cdot T_s & 0 \\\\\n",
    "0 & 0 & 1 & 0 & T_s \\\\\n",
    "0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrices\n",
    "**State covariance** :\n",
    "$ Q = \\begin{bmatrix}\n",
    "q_x & 0 & 0 & 0 & 0 \\\\\n",
    "0 & q_y & 0 & 0 & 0 \\\\\n",
    "0 & 0 & q_{\\gamma} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & q_v & 0 \\\\\n",
    "0 & 0 & 0 & 0 & q_{\\omega}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**Measurement covariance** : \n",
    "- Camera available : $R = \\begin{bmatrix}\n",
    "r_x & 0 & 0 & 0 & 0 \\\\\n",
    "0 & r_y & 0 & 0 & 0 \\\\\n",
    "0 & 0 & r_{\\gamma} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & r_v & 0 \\\\\n",
    "0 & 0 & 0 & 0 & r_{\\omega}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- Camera not available : \n",
    "When the camera in not available, the covariance of the measured positions and orientation are infinite. \n",
    "\n",
    "    Hence :\n",
    "$R_{\\text{no\\_camera}} = \\begin{bmatrix}\n",
    "∞ & 0 & 0 & 0 & 0 \\\\\n",
    "0 & ∞ & 0 & 0 & 0 \\\\\n",
    "0 & 0 & ∞ & 0 & 0 \\\\\n",
    "0 & 0 & 0 & r_v & 0 \\\\\n",
    "0 & 0 & 0 & 0 & r_{\\omega}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**Choice of the parameters:**\n",
    "\n",
    "We determined the parameters via a little experiment detailled in the file covariance_estimation.ipynb. This experiment is based on \"Solutions for exercise session 8\" from the course Basics of Mobile Robotics (MICRO-452) given by Francesco Mondada.\n",
    "\n",
    "First, we gave the instruction to the Thymio to go straight to a certain speed (50 thymiounit for both wheel) and measured the actual speed of the wheels.\n",
    "\n",
    "The avergage speed is calculated as : $avg\\_ speed =\\frac{v_{left}+v_{right}}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/speeds_for_covariance.png\" alt=\"Alt Text\" style=\"width: 50%;\"/>\n",
    "    <img src=\"images/Image_conversion.png\" alt=\"Alt Text\" style=\"width: 50%;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measured distance that the robot did is: d=280mm.\n",
    "\n",
    "Hence, we can calculate a conversion factor from Thymio-speed to mm/s : \n",
    "$$\n",
    "thymio\\_ speed\\_ to\\_ mms= \\frac{d}{T_s*nb_{timeSteps}*thymio\\_ speed}=0.4375$$\n",
    "\n",
    "Were $T_s$ is the time between 2 measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Covariance of v and w**\n",
    "\n",
    "To find the covariance of v and w we made 3 experciences. To have the most accurate results as possible, we did 3 times each experiments. The following results use the assumption that that half of the variance is caused by the perturbations and the other half by the measurement :\n",
    "\n",
    "\n",
    "**Experience 1 :** the thymio goes forward (speed (50,50))\n",
    "\n",
    ">Speed covariances found = 4.21 , 8.14 and 5.75 $[mm^2/s^2]$ \n",
    "\n",
    ">Angular speed covariances found = 0.0014, 0.0032 and 0.0024  $[mm^2/s^2]$ \n",
    "\n",
    "**Expercience 2 :** the thymio turn on itself (speed (50,0) as the speed of the wheels won't be negatives in our project)\n",
    "\n",
    ">Speed covariances found = 5.62, 5.81, 8.43\n",
    "\n",
    ">Angular speed covariances found = 0.0027, 0.0025, 0.0038\n",
    "\n",
    "**Expercience 3 :** the thymio follows a curved trajectory (speed (50,40))\n",
    "\n",
    ">Speed covariances found = 0.59, 0.58, 0.85\n",
    "\n",
    ">Angular speed covariances found = 0.0002, 0.0002, 0.0004\n",
    "\n",
    "In the table below one can find the means of the results \n",
    "\n",
    "| | Experience 1 | Experience 2 | Experience 3  |\n",
    "|-----------------|------------------|-----------------|-----------------|\n",
    "|$q_v=v_v [mm^2/s^2]$| 6.03 |  6.62  |  0.67  |\n",
    "|$q_w = r_w [rad^2/s^2]$|0.0023| 0.009| 0.00026 |\n",
    "\n",
    "From this table, we decided to used the following covariances : \n",
    "\n",
    "$q_v = r_v = 6 $ $mm^2/s^2$\n",
    "\n",
    "$ q_w = r_w = 0.009 $ $ rad^2/s^2$\n",
    "\n",
    "\n",
    "**Sate covariances of x, y and $\\gamma$**\n",
    "The covariances of x and y are the same. We chose the same arbitrary value as the one in the Exercise 8 :\n",
    "\n",
    "$q_x=q_y=q_p=0.04$\n",
    "\n",
    "**Measurement covariances of x, y and $\\gamma$**\n",
    "\n",
    "We arbitrarily chose the values $r_x = r_y = q_p = 0.001$. We selected small values because we want the filter to place significant trust in the position and angle measurements provided by the camera, as they are the most accurate measurements we can obtain for the robot.\n",
    "\n",
    "\n",
    "### Final covariance matrices\n",
    "$$\n",
    "Q = \\begin{bmatrix}  \n",
    "0.04 & 0 & 0 & 0 & 0 \\\\  \n",
    "0 & 0.04 & 0 & 0 & 0 \\\\  \n",
    "0 & 0 & 0.01 & 0 & 0 \\\\  \n",
    "0 & 0 & 0 & 6 & 0 \\\\  \n",
    "0 & 0 & 0 & 0 & 0.009  \n",
    "\\end{bmatrix}, R = \\begin{bmatrix}  \n",
    "0.001 & 0 & 0 & 0 & 0 \\\\  \n",
    "0 & 0.001 & 0 & 0 & 0 \\\\  \n",
    "0 & 0 & 0.001 & 0 & 0 \\\\  \n",
    "0 & 0 & 0 & 6 & 0 \\\\  \n",
    "0 & 0 & 0 & 0 & 0.009  \n",
    "\\end{bmatrix}, R_{\\text{no\\_camera}} = \\begin{bmatrix}  \n",
    "\\infty & 0 & 0 & 0 & 0 \\\\  \n",
    "0 & \\infty & 0 & 0 & 0 \\\\  \n",
    "0 & 0 & \\infty & 0 & 0 \\\\  \n",
    "0 & 0 & 0 & 6 & 0 \\\\  \n",
    "0 & 0 & 0 & 0 & 0.009  \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation/measurement model \n",
    "\n",
    "The observation model is given by : \n",
    "$$\n",
    "y_k = H \\cdot x_k \n",
    "$$\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "y_1^k \\\\\n",
    "y_2^k \\\\\n",
    "y_3^k \\\\\n",
    "y_4^k \\\\\n",
    "y_5^k\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix} \n",
    "x_{k} \\\\\n",
    "y_{k} \\\\\n",
    "\\gamma_{k} \\\\\n",
    "v_{k}\\\\\n",
    "\\omega_{k}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The observations are given by the camera for $x_{k}$, $y_{k}$ and $\\gamma_{k}$ and by the odometry for $v_{k}$ and $\\omega_{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Control Module\n",
    "\n",
    "# Motion Control Module\n",
    "\n",
    "Once the vision module is implemented to send the robot's position and angle, and the optimal path is computed, the motion control module comes into play to guide the robot toward its goal efficiently and reliably. Our A* algorithm computes a highly precise path, which we segment into closely spaced waypoints to ensure smooth and accurate motion. To navigate these waypoints, we implemented a simplified version of the Astolfi controller, which is ideal for the Thymio's non-holonomic differential drive system. Due to the short distances between waypoints, we implemented a more simplified version of the Astolfi controller then the one seen in the course, omitting the beta angle correction. The controller focuses on minimizing two critical variables: the distance to the goal ($\\rho$) and the alignment error ($\\alpha$), ensuring the robot aligns itself with its path and reaches each waypoint effectively.\n",
    "\n",
    "![Controller Diagram](/controller.jpg)\n",
    "\n",
    "The distance to the goal, $\\rho$, is calculated as:\n",
    "\n",
    "$$\n",
    "\\rho = \\sqrt{(x_{goal} - x_{robot})^2 + (y_{goal} - y_{robot})^2}\n",
    "$$\n",
    "\n",
    "This represents the Euclidean distance between the robot's position and the target waypoint. \n",
    "\n",
    "The alignment error, $\\alpha$, is defined as:\n",
    "\n",
    "$$\n",
    "\\alpha = \\arctan2(dy, dx) - \\theta\n",
    "$$\n",
    "\n",
    "Here, $dx = x_{goal} - x_{robot}$ and $dy = y_{goal} - y_{robot}$, while $\\theta$ is the robot's current orientation. The term $\\arctan2(dy, dx)$ gives the direction to the waypoint relative to the robot's current position, and subtracting $\\theta$ calculates the angular difference. To adapt to the inverted y-axis of the camera’s coordinate frame, we flipped the signs of the y-coordinates in our implementation, ensuring consistency in how the controller interprets these values.\n",
    "\n",
    "The controller generates two outputs: linear velocity ($v$) and angular velocity ($\\omega$), which are computed using proportional control laws. These are defined as:\n",
    "\n",
    "$$\n",
    "v = K_\\rho \\cdot \\rho\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\omega = K_\\alpha \\cdot \\alpha\n",
    "$$\n",
    "\n",
    "Here, $K_\\rho > 0$ and $K_\\alpha > 0$ are gain parameters tuned for stability. The gain $K_\\alpha$ is chosen to be greater than $K_\\rho$ ($K_\\alpha > K_\\rho$) to ensure that the robot prioritizes alignment before moving forward, preventing overshooting or instability. Both $K_\\rho$ and $K_\\alpha$ must be positive to ensure the robot moves toward the goal while aligning itself correctly, maintaining stability by progressively reducing position and angular errors, as guaranteed by Lyapunov’s method.\n",
    "\n",
    "To determine these gains, we used a simple method for initial parameter estimation. Starting with a straightforward scenario where the robot moves in a straight line, such as from (0,0,0) to (x,0), we defined a desired wheel speed for this motion and estimated the corresponding $v$, $\\rho$ and estimated distance traveled. From this, we had an idea of starting values for $K_\\rho$ to produce the required linear velocity. For $K_\\alpha$, we chose a value slightly larger than $K_\\rho$ to ensure the robot prioritizes angular alignment and stability reason. These initial guesses were then refined through empirical tuning, where we tested different scenarios, like turning or navigating along curved paths, and adjusted the parameters until the robot exhibited stable and smooth behavior.This iterative approach proved practical and effective for our implementation.\n",
    "\n",
    "These velocities are then mapped to the Thymio's motor speeds in `compute_motor_speeds` function using the differential drive equations:\n",
    "\n",
    "$$\n",
    "v_{right} = v + \\omega \\cdot d\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_{left} = v - \\omega \\cdot d\n",
    "$$\n",
    "\n",
    "`compute_motor_speeds` also makes sure the velocities don't exceed a certain max_speed we precised, and converts them to thymio speed units.\n",
    "\n",
    "Here, $d$ represents the distance between the robot's wheels. This mapping ensures that the Thymio can translate the computed velocities into physical movement.\n",
    "\n",
    "A certain threshold is defined for the distance to the goal ($\\rho$), which determines when the Thymio has successfully reached its target waypoint. By continuously recalculating the distance and alignment errors as the robot moves, the controller dynamically adjusts the velocities, enabling the Thymio to navigate its path with accuracy and stability. This method combines the precision of proportional control with the adaptability required for real-world navigation.\n",
    "\n",
    "All relavant functions to this part are in the `Control.py` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Map :**\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/decision_map.jpg\" alt=\"Alt Text\" style=\"width: 50%;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports from each module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Modules.Control as Control\n",
    "import matplotlib.pyplot as plt\n",
    "import Modules.Path_planning as Path_planning\n",
    "import tdmclient.notebook\n",
    "import Modules.ThymioCommands as ThymioCommands\n",
    "import Modules.LocalNav as LocalNav\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import Modules.Kalman as Kalman\n",
    "from Modules.ThymioVision import ThymioVision # Computer vision package\n",
    "from scipy.ndimage import distance_transform_edt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdmclient.notebook\n",
    "import time\n",
    "#fonctions de Julie \n",
    "@tdmclient.notebook.sync_var \n",
    "def set_speed(right_speed,left_speed):\n",
    "    global motor_right_target, motor_left_target\n",
    "    motor_right_target=right_speed\n",
    "    motor_left_target=left_speed\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def stop_thymio():\n",
    "    global motor_right_target,motor_left_target\n",
    "    motor_right_target=0\n",
    "    motor_left_target=0\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def get_speed():\n",
    "    global motor_right_speed, motor_left_speed\n",
    "    return motor_right_speed,motor_left_speed\n",
    "\n",
    "def speed_convesion(r_speed,l_speed):\n",
    "    thymio_speed_to_mms = 0.4375 # value found in covariance_estimation\n",
    "\n",
    "    #odometry \n",
    "    avg_thymio_speed = (r_speed + l_speed) / 2\n",
    "    speed = avg_thymio_speed * thymio_speed_to_mms # [mm/s]\n",
    "    return speed\n",
    "\n",
    "def angular_vel_conversion(r_speed,l_speed):\n",
    "    d = 95 # distance between the 2 wheels [mm]\n",
    "    thymio_speed_to_mms = 0.4375 # value found in covariance_estimation\n",
    "    \n",
    "    difference_speed = l_speed - r_speed\n",
    "    omega = difference_speed * thymio_speed_to_mms / d # [rad/s]\n",
    "\n",
    "    return omega\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def get_proximity_values():\n",
    "    global prox_horizontal\n",
    "    prox = prox_horizontal\n",
    "    prox_front = prox[2]\n",
    "    prox_left = prox[0]\n",
    "    prox_left_front = prox[1]\n",
    "    prox_right_front = prox[3]\n",
    "    prox_right = prox[4]\n",
    "    return prox_front, prox_left, prox_left_front, prox_right_front, prox_right\n",
    "\n",
    "def avoid_obstacle(vc, duration = 2, decision=\"right\"):\n",
    "    \"\"\"\n",
    "    Perform a simplified L-shaped maneuver for obstacle avoidance using time.sleep.\n",
    "    Args:\n",
    "        decision (str): Turning direction (\"right\" or \"left\").\n",
    "    \"\"\"\n",
    "    print(\"Starting obstacle avoidance maneuver...\")\n",
    "\n",
    "    # Phase 1: Turn away from the obstacle\n",
    "    print(f\"Phase 1: Turning {decision} away from the obstacle.\")\n",
    "    curr_time = time.time()\n",
    "    if decision == \"right\":\n",
    "        set_speed(-100, 100)  # Turn right\n",
    "    elif decision == \"left\":\n",
    "        set_speed(100, -100)  # Turn left\n",
    "    while time.time() - curr_time < duration:\n",
    "        ret,frame = vc.read()\n",
    "        if ret:\n",
    "            cv2.imshow(\"Camera view\", frame)\n",
    "        key = cv2.waitKey(50)\n",
    "    \n",
    "    # Phase 2: Move forward to clear the obstacle\n",
    "    curr_time = time.time()\n",
    "    print(\"Phase 2: Moving forward to clear the obstacle.\")\n",
    "    set_speed(100, 100)  \n",
    "    while time.time() - curr_time < duration+1:\n",
    "        ret,frame = vc.read()\n",
    "        if ret:\n",
    "            cv2.imshow(\"Camera view\", frame)\n",
    "        key = cv2.waitKey(50)\n",
    "\n",
    "    # Phase 3: Turn back to original direction\n",
    "    curr_time = time.time()\n",
    "    print(f\"Phase 3: Returning to original direction (opposite of {decision}).\")\n",
    "    if decision == \"right\":\n",
    "        set_speed(100, -100)  # Turn left\n",
    "    elif decision == \"left\":\n",
    "        set_speed(-100, 100)  # Turn right\n",
    "    while time.time() - curr_time < duration:\n",
    "        ret,frame = vc.read()\n",
    "        if ret:\n",
    "            cv2.imshow(\"Camera view\", frame)\n",
    "        cv2.waitKey(50)\n",
    "\n",
    "    # Stop the robot\n",
    "    stop_thymio()\n",
    "    print(\"Obstacle avoidance completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin the thymio, calibrate for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global leds_circle\n",
    "leds_circle=[32,32,0,0,0,0,0,32] # Turn on thymio lights for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tdmclient.notebook.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThymioVision.calibrateCameraPos(0) #Calibrate the corners to the map.\n",
    "frame = ThymioVision.captureImageOnCommand(0) # Capture the image to define the map\n",
    "edges = ThymioVision.getEdges(frame, verbose=True)\n",
    "start = ThymioVision.getThymioPose(frame, verbose=True)\n",
    "end = ThymioVision.detectGoal(frame, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = ThymioVision.getMap(frame, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs for the path_planning module\n",
    "obstacles = map[0]\n",
    "start = map[1]\n",
    "end = map[2]\n",
    "# adjusting x/y to row/col -> flip them.\n",
    "correctedStart = [start[1], start[0]]\n",
    "correctedGoal = [end[1], end[0]]\n",
    "\n",
    "# Determine best path from map\n",
    "path = Path_planning.A_star(obstacles, correctedStart, correctedGoal)\n",
    "\n",
    "# Break the path into waypoints that the Thymio can follow.\n",
    "# Swap back to pixel coordinates\n",
    "swapped = Path_planning.swap_path_coordinates(path)\n",
    "waypoints = []\n",
    "for point in swapped:\n",
    "    x, y = ThymioVision.pixelToRealSpace(point)\n",
    "    waypoints.append((x*10, y*10)) #convert to mm!\n",
    "waypoints = Control.segment_path(waypoints, step=40)\n",
    "\n",
    "lx = []\n",
    "ly = []\n",
    "for wp in path:\n",
    "    lx.append(wp[0])\n",
    "    ly.append(wp[1])\n",
    "\n",
    "# Display path on frame\n",
    "plt.imshow(frame)\n",
    "plt.plot(ly, lx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_map = distance_transform_edt(map[0] == 0)\n",
    "\n",
    "flipped_dist_map = np.flipud(dist_map)\n",
    "\n",
    "# Plot the distance transform\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(dist_map.T, origin='lower', cmap='viridis', extent=(0, 100, 0, 100))\n",
    "plt.colorbar(label=\"Distance to Nearest Wall\")\n",
    "plt.title(\"Distance Transform of the Map\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Movement Loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# BEGIN CAMERA CAPTURE\n",
    "cv2.namedWindow(\"Camera view\")\n",
    "vc=cv2.VideoCapture(0)\n",
    "print(\"vc captured\")\n",
    "\n",
    "###################### INITIALIZE STATE FROM MAP READ ######################\n",
    "\n",
    "obstSpeedGain = [2, 1, -1, -1.5, -2.5]  # Gains for obstacle avoidance\n",
    "OBSTACLE_THRESH_LOW_FRONT = 8 # Threshold to stop avoiding\n",
    "OBSTACLE_THRESH_HIGH_FRONT = 30 # Threshold to start avoiding\n",
    "OBSTACLE_THRESH_HIGH_SIDE = 40 # Threshold to start avoiding\n",
    "thymio_state = 0  # 0 = Goal tracking, 1 = Obstacle avoidance, 2 = kidnapping\n",
    "goal_tolerance = 30\n",
    "\n",
    "\n",
    "state_est=[0,0,0,0,0] \n",
    "ret,frame=vc.read()\n",
    "if ret:\n",
    "    pose = ThymioVision.getThymioPose(frame) #pixel location of thymio\n",
    "    obs_camera = ThymioVision.pixelToRealSpace(pose[0:2]) #returns cm, *10 to get mm\n",
    "    obs_camera = (obs_camera[0]*10, obs_camera[1]*10, pose[2]) #thymio position in mm to give to kalman filter\n",
    "    state_est=[obs_camera[0],obs_camera[1],obs_camera[2],0,0]   \n",
    "P_est=np.diag([0.1,0.1,0.1,0.1,0.1])\n",
    "\n",
    "\n",
    "###################### LOGGING DATA ######################\n",
    "trajectory = []  # To store [x, y] positions\n",
    "trajectory.append(obs_camera[0:2])\n",
    "camera_trajectory = [] #trajectory mapped by camera\n",
    "trajectory = []  # Logs (x, y) positions\n",
    "metrics_rho = []  # Logs for distance to goal\n",
    "metrics_alpha = []  # Logs for angle to goal\n",
    "time_prev = time.time()\n",
    "\n",
    "###################### BEGIN MOTION LOOP ######################\n",
    "while len(waypoints) > 0: # while we have goals in life\n",
    "    actual_time = time.time()\n",
    "    Ts = actual_time - time_prev\n",
    "    if Ts < 0.15:\n",
    "        time.sleep(0.15-Ts)\n",
    "    time_prev = actual_time\n",
    "\n",
    "    waypoint = waypoints[0] # current goal\n",
    "\n",
    "    ################### SENSOR READS ###################\n",
    "    ret,frame=vc.read()\n",
    "    if ret:\n",
    "        pose = ThymioVision.getThymioPose(frame)#pixel location of thymio\n",
    "        if pose[0] is not None:\n",
    "            obs_camera = ThymioVision.pixelToRealSpace(pose[0:2]) #returns cm, *10 to get mm\n",
    "            obs_camera = (obs_camera[0]*10, obs_camera[1]*10, pose[2]) #thymio position in mm to give to kalman filter\n",
    "            camera_trajectory.append(obs_camera) #position in mm appended to log\n",
    "            arrowStart = pose[0:2]\n",
    "            arrowEnd = (int(pose[0] + 50 * np.cos(pose[2])), int(pose[1] - 50 * np.sin(pose[2])))\n",
    "            cv2.arrowedLine(frame, arrowStart, arrowEnd, color=(0,0,255), thickness=3) #camera pos\n",
    "            for i in range(len(swapped)-1):\n",
    "                cv2.line(frame, swapped[i], swapped[i+1], color=(255, 0 , 50), thickness=2)\n",
    "            # convert trajectory onto map\n",
    "        if len(trajectory) > 0:\n",
    "            lastPos = trajectory[-1]\n",
    "            lastPos = (lastPos[0]/10, lastPos[1]/10)\n",
    "            pixelPred = ThymioVision.realSpaceToPixel(lastPos)\n",
    "            cv2.circle(frame, (pixelPred), 3, (0,255,0),-1) #est pos\n",
    "        cv2.imshow(\"Camera view\", frame)\n",
    "        key = cv2.waitKey(50)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    if not ret or pose[0] is None:\n",
    "        camera_state=False\n",
    "    else:\n",
    "        camera_state=True\n",
    "    print(camera_state)\n",
    "\n",
    "    prox_front, prox_left, prox_left_front, prox_right_front, prox_right = get_proximity_values()\n",
    "    proximity_values = [prox_left, prox_left_front, prox_front, prox_right_front, prox_right]\n",
    "\n",
    "    ###################### CHECK THYMIO STATE ######################\n",
    "\n",
    "    # Check for obstacle state transitions\n",
    "    if thymio_state == 0:\n",
    "        # Switch to obstacle avoidance if an obstacle is detected\n",
    "        if any((prox // 100) > OBSTACLE_THRESH_HIGH_FRONT for prox in proximity_values[1:4]) or prox_right//100 > OBSTACLE_THRESH_HIGH_FRONT or prox_left//100 > OBSTACLE_THRESH_HIGH_FRONT:\n",
    "            thymio_state = 1\n",
    "        elif len(trajectory) >= 1 and ((obs_camera[0]-trajectory[-1][0])**2 + (obs_camera[1]-trajectory[-1][1])**2) > 500:\n",
    "            print(obs_camera)\n",
    "            print(trajectory[-1])\n",
    "            # if change between states is too rapid, we have been kidnapped. recalculate and attempt again\n",
    "            thymio_state = 2\n",
    "            continue\n",
    "    elif thymio_state == 1:\n",
    "    # Switch back to goal tracking if obstacles are cleared\n",
    "        if all((prox // 100) < OBSTACLE_THRESH_LOW_FRONT for prox in proximity_values):\n",
    "            print(\"Obstacle avoided\")\n",
    "            #set_speed(20, 20)  #Continue getting away from obstacle for a short time ?   \n",
    "            #stop_thymio() #idk if i keep it \n",
    "            thymio_state = 2 # set state to \"kidnapped.\" Will recalculate the path and continue.\n",
    "\n",
    "    ###################### GLOBAL NAVIGATION ######################\n",
    "    if thymio_state == 0:\n",
    "        print(\"Global Nav\")\n",
    "        # input_left_speed, input_right_speed, reached , v_f , omega_f  = Control.move_to_waypoint(state_est, waypoint, goal_tolerance)\n",
    "        # input_left_speed, input_right_speed, reached , v_f , omega_f  = Control.move_to_waypoint(obs_camera, waypoint, goal_tolerance)\n",
    "        if not camera_state:\n",
    "            input_left_speed, input_right_speed, reached , v_f , omega_f  = Control.move_to_waypoint(state_est, waypoint, goal_tolerance*2)\n",
    "            set_speed(int(input_right_speed/2), int(input_left_speed/2))\n",
    "        else:\n",
    "            input_left_speed, input_right_speed, reached , v_f , omega_f  = Control.move_to_waypoint(state_est, waypoint, goal_tolerance)\n",
    "            set_speed(input_right_speed, input_left_speed)\n",
    "    ###################### LOCAL NAVIGATION ######################   \n",
    "    elif thymio_state == 1:\n",
    "        print(\"Avoidance\")\n",
    "        decision, left_dist, right_dist = LocalNav.check_sides(state_est[0:3], flipped_dist_map)\n",
    "        print(f\"Turning decision: {decision}, Left Distance: {left_dist}, Right Distance: {right_dist}\")\n",
    "        if not(prox_right//100 > OBSTACLE_THRESH_HIGH_SIDE) and not (prox_left//100 > OBSTACLE_THRESH_HIGH_SIDE):\n",
    "            avoid_obstacle(vc=vc, duration=2, decision=decision)\n",
    "        else:\n",
    "            avoid_obstacle(vc=vc, duration=1, decision=decision)\n",
    "\n",
    "    ###################### KIDNAPPING RECALCULATION ###################### \n",
    "    else:\n",
    "        print(\"Kidnapped\") ##TODO FIX NO PATH FOUND ERROR\n",
    "        # stop the thymio\n",
    "        input_left_speed, input_right_speed, reached , v_f , omega_f = 0, 0, False, 0, 0\n",
    "        set_speed(input_right_speed, input_left_speed)\n",
    "        #if the camera is reading, update the path\n",
    "        if camera_state:\n",
    "            # convert trajectory onto map\n",
    "            if len(trajectory) > 0:\n",
    "                lastPos = trajectory[-1]\n",
    "                lastPos = (lastPos[0]/10, lastPos[1]/10)\n",
    "                pixelPred = ThymioVision.realSpaceToPixel(lastPos)\n",
    "                cv2.circle(frame, (pixelPred), 3, (0,255,0),-1) #est pos\n",
    "            cv2.imshow(\"Camera view\", frame)\n",
    "            path = None\n",
    "            while not path:\n",
    "                ret,frame=vc.read()\n",
    "                if ret:\n",
    "                    pose = ThymioVision.getThymioPose(frame)#pixel location of thymio\n",
    "                    if pose[0] is not None:\n",
    "                        path = Path_planning.A_star(obstacles, (pose[1],pose[0]), correctedGoal)\n",
    "                    cv2.imshow(\"Camera view\", frame)\n",
    "            swapped = Path_planning.swap_path_coordinates(path)\n",
    "            waypoints = []\n",
    "            for point in swapped:\n",
    "                x, y = ThymioVision.pixelToRealSpace(point)\n",
    "                waypoints.append((x*10, y*10)) #convert to mm!\n",
    "            waypoints = Control.segment_path(waypoints, step=40)\n",
    "            trajectory.append(obs_camera[0:2]) #append most recent location\n",
    "            thymio_state = 0 # go back to global nav\n",
    "\n",
    "        \n",
    "    ################### UPDATE STATE ESTIMATE ###################\n",
    "\n",
    "    # Control input\n",
    "    input_v = v_f #Ca jsp si je remplace par actual_v et actual_w\n",
    "    input_w = omega_f\n",
    "    control_vect_prev = [input_v, input_w]\n",
    "    \n",
    "    r_speed_odo, l_speed_odo = get_speed()\n",
    "    actual_v = speed_convesion(r_speed_odo, l_speed_odo)\n",
    "    actual_w = angular_vel_conversion(r_speed_odo, l_speed_odo)\n",
    "    obs_odometry = [actual_v, actual_w]\n",
    "\n",
    "    # Update state with Kalman filter\n",
    "    state_est, P_est = Kalman.ekf(state_est, control_vect_prev, P_est, obs_camera, obs_odometry, camera_state,Ts)\n",
    "    state_est[2]=Control.normalize_angle(state_est[2])\n",
    "    # Log trajectory\n",
    "    trajectory.append((state_est[0], state_est[1]))\n",
    "    \n",
    "    # Calculate and log metrics\n",
    "    rho = np.sqrt((waypoint[0] - state_est[0])**2 + (waypoint[1] - state_est[1])**2)\n",
    "    alpha = Control.normalize_angle(np.arctan2(waypoint[1] - state_est[1], waypoint[0] - state_est[0]) - state_est[2])\n",
    "    metrics_rho.append(rho)\n",
    "    metrics_alpha.append(alpha)\n",
    "    \n",
    "    # Print updated state\n",
    "    # print(ThymioVision.pixelToRealSpace(pose[0:2]))\n",
    "    print(f\"Position Actuelle camera  : x = {obs_camera[0]}, y = {obs_camera[1]}, theta = {obs_camera[2]}\") \n",
    "    print(f\"Position Actuelle odometry: x = {state_est[0]}, y = {state_est[1]}, theta = {state_est[2]}\") \n",
    "\n",
    "    if reached:\n",
    "        waypoints.pop(0)\n",
    "        print(f\"Reached waypoint: {waypoint}\")\n",
    "        # Stop the robot at the waypoint\n",
    "        stop_thymio()\n",
    "\n",
    "vc.release()\n",
    "stop_thymio()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Trajectory completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video of our project results : https://drive.google.com/file/d/1tEe2iRlEP_2pRoXrDznQHucG7x7W3pVn/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "- Course \"Basics of Mobile Robotics\" by Francesco Mondada\n",
    "- ChatGPT\n",
    "\n",
    "### Global Navigation\n",
    "- *\"Space Robotics Systems\"*, delivered by Prof. Antonio Genova and Dr. Edoardo Del Vecchio from the Department of Mechanical and Aerospace Engineering, Sapienza University of Rome. \n",
    "\n",
    "### Filtering \n",
    "- \"Lesson 7/8- Uncertainties\" from the course Basics of Mobile Robotics (MICRO-452) given by Francesco Mondada.\n",
    "- \"Solutions for exercise session 8\" from the course Basics of Mobile Robotics (MICRO-452) given by Francesco Mondada.\n",
    "- Automatic Addison https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/\n",
    "- Wikipedia page of the Extended Kalman Filter : https://en.wikipedia.org/wiki/Extended_Kalman_filter#Discrete-time_predict_and_update_equations\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
